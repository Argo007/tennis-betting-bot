name: Matrix Backtest (Kelly + TE)

on:
  workflow_dispatch:
    inputs:
      dataset:
        description: "CSV path to use (leave blank to auto-pick)"
        required: false
        default: ""
      bands:
        description: "Edge bands e.g. 2.0,2.6|2.6,3.2|3.2,4.0"
        required: true
        default: "2.0,2.6|2.6,3.2|3.2,4.0"
      staking:
        description: "Staking model"
        required: true
        type: choice
        options: [kelly, flat]
        default: kelly
      kelly_scale:
        description: "Kelly scaler (0.5 = half Kelly)"
        required: true
        default: "0.5"
      bankroll:
        description: "Starting bankroll (units)"
        required: true
        default: "1000"
      min_edge:
        description: "Minimum true edge to include (e.g., 0.005)"
        required: true
        default: "0.005"

permissions:
  contents: read

concurrency:
  group: matrix-backtest-${{ github.ref }}
  cancel-in-progress: false

jobs:
  backtest:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # Decide which dataset to use, in priority order.
      # 1) user input (if provided)
      # 2) outputs/edge_enriched.csv
      # 3) outputs/prob_enriched.csv
      # 4) data/raw/vigfree_matches.csv
      # 5) data/raw/odds/sample_odds.csv (synthetic demo)
      - name: Resolve dataset path
        id: ds
        shell: bash
        run: |
          set -euo pipefail
          pick_if_exists () { [ -n "${1:-}" ] && [ -f "$1" ] && echo "$1"; }

          CANDIDATES=()

          # From user input (if any)
          if [ -n "${{ github.event.inputs.dataset }}" ]; then
            CANDIDATES+=("${{ github.event.inputs.dataset }}")
          fi

          # Auto candidates
          CANDIDATES+=(
            "outputs/edge_enriched.csv"
            "outputs/prob_enriched.csv"
            "data/raw/vigfree_matches.csv"
            "data/raw/odds/sample_odds.csv"
          )

          DATASET=""
          for c in "${CANDIDATES[@]}"; do
            if [ -f "$c" ]; then DATASET="$c"; break; fi
          done

          if [ -z "$DATASET" ]; then
            echo "ERROR: No usable dataset found." >&2
            exit 1
          fi

          echo "dataset=$DATASET" >> "$GITHUB_OUTPUT"
          echo "Resolved dataset: $DATASET"

      # Ensure we have a file with columns: oa, ob, pa, pb.
      # If the chosen dataset already has pa/pb -> copy to outputs/prob_enriched.csv
      # Else if it has vigfree probs (prob_a_vigfree/prob_b_vigfree) -> rename to pa/pb
      # Else if it only has oa/ob -> derive pa/pb from implied probs (no vig removal)
      - name: Normalize to prob_enriched.csv (adds pa/pb if missing)
        run: |
          set -euo pipefail
          python - << 'PY'
          import pandas as pd, sys, os
          src = r"""${{ steps.ds.outputs.dataset }}"""
          df = pd.read_csv(src)

          def has(cols): return all(c in df.columns for c in cols)

          # Ensure odds columns are named oa/ob
          rename_map = {
              'odds_a':'oa','odds_b':'ob','oa':'oa','ob':'ob'
          }
          df.rename(columns=rename_map, inplace=True)

          # Case 1: already has pa/pb
          if has(['pa','pb']):
              out = 'outputs/prob_enriched.csv'
              os.makedirs(os.path.dirname(out), exist_ok=True)
              df.to_csv(out, index=False)
              print(out)
              sys.exit(0)

          # Case 2: vigfree prob columns present -> map to pa/pb
          for ca in ['prob_a_vigfree','prob_a_vig_free','pa_vigfree','p_a']:
              if ca in df.columns:
                  df['pa'] = df[ca]; break
          for cb in ['prob_b_vigfree','prob_b_vig_free','pb_vigfree','p_b']:
              if cb in df.columns:
                  df['pb'] = df[cb]; break

          if 'pa' in df.columns and 'pb' in df.columns:
              out = 'outputs/prob_enriched.csv'
              os.makedirs(os.path.dirname(out), exist_ok=True)
              df.to_csv(out, index=False)
              print(out)
              sys.exit(0)

          # Case 3: derive pa/pb from odds if available (fair = 1/odds)
          if has(['oa','ob']):
              df['pa'] = 1.0/df['oa']
              df['pb'] = 1.0/df['ob']
              # Normalize to sum to 1 when both present
              s = df[['pa','pb']].sum(axis=1)
              df.loc[s>0, ['pa','pb']] = df.loc[s>0, ['pa','pb']].div(s[s>0], axis=0)

              out = 'outputs/prob_enriched.csv'
              os.makedirs(os.path.dirname(out), exist_ok=True)
              df.to_csv(out, index=False)
              print(out)
              sys.exit(0)

          raise SystemExit("No way to obtain pa/pb from the selected dataset.")
          PY

      - name: Enrich edges (EdgeSmith)
        run: |
          set -euo pipefail
          python scripts/edge_smith_enrich.py \
            --input outputs/prob_enriched.csv \
            --output outputs/edge_enriched.csv \
            --min-edge "${{ github.event.inputs.min_edge }}"

      - name: Run matrix backtest
        run: |
          set -euo pipefail
          python scripts/run_matrix_backtest.py \
            --dataset outputs/edge_enriched.csv \
            --min-edge "${{ github.event.inputs.min_edge }}" \
            --staking "${{ github.event.inputs.staking }}" \
            --kelly-scale "${{ github.event.inputs.kelly_scale }}" \
            --bankroll "${{ github.event.inputs.bankroll }}" \
            --bands "${{ github.event.inputs.bands }}"

          # Collect outputs in a single folder for upload
          mkdir -p results/backtests
          for p in \
            backtests/summary.csv \
            backtests/params_cfg1.json \
            backtests/logs/picks_cfg1.csv \
            backtests/_diagnostics.json \
            outputs/prob_enriched.csv \
            outputs/edge_enriched.csv
          do
            [ -f "$p" ] && cp --parents "$p" results/backtests/ 2>/dev/null || true
          done
          if [ -d backtests/logs ]; then
            mkdir -p results/backtests/logs
            cp -r backtests/logs/* results/backtests/logs/ || true
          fi

      - name: Generate HTML report (if available)
        run: |
          set -euo pipefail
          if [ -f scripts/generate_report.py ]; then
            python scripts/generate_report.py \
              --summary backtests/summary.csv \
              --params backtests/params_cfg1.json \
              --dataset outputs/prob_enriched.csv \
              --out docs/backtests/index.html || true
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: matrix-backtest-${{ github.run_id }}
          path: |
            results/backtests/**
            backtests/summary.csv
            backtests/params_cfg1.json
            backtests/logs/**
            outputs/prob_enriched.csv
            outputs/edge_enriched.csv
            docs/backtests/index.html
          if-no-files-found: warn

