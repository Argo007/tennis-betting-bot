name: Courtsense (Build + Normalize + Backtest)

on:
  schedule:
    - cron: "0 8 * * *" # Daily at 08:00 UTC
  workflow_dispatch:
    inputs:
      years_csv:
        description: "Years (space-separated)"
        required: false
        default: "2021 2022 2023 2024"
      do_backtest:
        description: "Run TE8 backtest?"
        required: true
        type: choice
        default: "yes"
        options: ["yes","no"]
      start_date:
        description: "Backtest start (YYYY-MM-DD)"
        required: false
        default: "2021-01-01"
      end_date:
        description: "Backtest end (YYYY-MM-DD)"
        required: false
        default: "2024-12-31"
      te8_dog:
        description: "TE8 threshold (dogs)"
        required: false
        default: "0.60"
      te8_fav:
        description: "TE8 threshold (favs)"
        required: false
        default: "0.50"
      bands:
        description: "Odds bands: dog=min,max;fav=min,max"
        required: false
        default: "dog=2.20,4.50;fav=1.15,2.00"
      money:
        description: "Money: bankroll=1000;stake_unit=100;dog_cap=0.25"
        required: false
        default: "bankroll=1000;stake_unit=100;dog_cap=0.25"
      tuners:
        description: "Tuners: surface_boost=0.05;recent_form_weight=0.30;injury_penalty=0.15"
        required: false
        default: "surface_boost=0.05;recent_form_weight=0.30;injury_penalty=0.15"

env:
  TZ: Europe/Amsterdam
  YEARS_CSV: ${{ github.event.inputs.years_csv || '2021 2022 2023 2024' }}

permissions:
  contents: read

concurrency:
  group: courtsense
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Prepare requirements
        run: |
          printf "pandas\nnumpy\nrequests\nopenpyxl\nrapidfuzz\npython-dateutil\nUnidecode\n" > requirements.txt

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "requirements.txt"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Optional: seed tiny odds so you always see something
      - name: Seed sample odds if none present
        run: |
          mkdir -p data/raw/odds
          python - <<'PY'
          from pathlib import Path
          d=Path("data/raw/odds")
          has= any(d.glob("*.csv")) or any(d.glob("*.xlsx")) or any(d.glob("*.xls"))
          if not has:
            (d/"atp_sample_odds.csv").write_text(
              "date,player_a,player_b,odds_a,odds_b\n"
              "2021-09-12,Daniil Medvedev,Novak Djokovic,2.60,1.55\n"
              "2022-07-10,Novak Djokovic,Nick Kyrgios,1.30,3.60\n"
              "2023-06-11,Novak Djokovic,Casper Ruud,1.33,3.40\n"
              "2024-01-28,Jannik Sinner,Daniil Medvedev,1.85,2.05\n", encoding="utf-8")
            (d/"wta_sample_odds.csv").write_text(
              "date,player_a,player_b,odds_a,odds_b\n"
              "2021-06-12,Barbora Krejcikova,Anastasia Pavlyuchenkova,1.75,2.10\n"
              "2023-07-15,Marketa Vondrousova,Ons Jabeur,2.30,1.65\n"
              "2024-07-13,Barbora Krejcikova,Jasmine Paolini,1.60,2.40\n", encoding="utf-8")
            print("Seeded sample odds.")
          else:
            print("Odds exist; no seed.")
          PY

      - name: Normalize local odds (names & snap dates)
        run: |
          python scripts/normalize_odds.py --years "${{ env.YEARS_CSV }}" --snap-dates yes || true

      - name: Build dataset (Sackmann + normalized odds) and summaries
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, io, sys, glob, json, re, time
          from datetime import datetime
          import requests, pandas as pd
          from rapidfuzz import fuzz, process
          from unidecode import unidecode

          YEARS=[int(y) for y in os.getenv("YEARS_CSV","2024").split() if y.strip().isdigit()]
          ODDS_PRIMARY="data/raw/odds_normalized"
          ODDS_FALLBACK="data/raw/odds"
          OUT_CSV="data/historical_matches.csv"
          ATP_ELO="data/atp_elo.csv"; WTA_ELO="data/wta_elo.csv"
          BUILD_MD="data/build_summary.md"; META="data/dataset_info.json"
          OUTPUT_COLS=["date","tour","tournament","round","player","opponent",
                       "odds","opp_odds","result","elo_player","elo_opponent","surface","source"]

          def norm(s):
              s=unidecode(str(s)).lower()
              s=re.sub(r"[^a-z ]"," ",s)
              return re.sub(r"\s+"," ",s).strip()

          def fetch_csv(url):
              for wait in (0,1.5,3.0):
                  try:
                      r=requests.get(url, timeout=12); r.raise_for_status()
                      import io; return pd.read_csv(io.StringIO(r.text))
                  except Exception: time.sleep(wait)
              raise RuntimeError(f"Fetch failed: {url}")

          def fetch_sack():
              frames=[]; pulled={}
              for tour, repo in [("ATP","tennis_atp"),("WTA","tennis_wta")]:
                  for y in YEARS:
                      url=f"https://raw.githubusercontent.com/JeffSackmann/{repo}/master/{tour.lower()}_matches_{y}.csv"
                      df=fetch_csv(url); df["tour"]=tour; frames.append(df); pulled[(tour,y)]=len(df)
              res=pd.concat(frames, ignore_index=True)
              res["date"]=pd.to_datetime(res["tourney_date"].astype(str), format="%Y%m%d", errors="coerce").dt.normalize()
              keep=["tour","tourney_name","date","surface","round","winner_name","loser_name"]
              for k in keep:
                  if k not in res.columns: res[k]=pd.NA
              res=res[keep].copy()
              res["w_norm"]=res["winner_name"].map(norm); res["l_norm"]=res["loser_name"].map(norm)
              res["pair"]=res.apply(lambda r: " vs ".join(sorted([r["w_norm"],r["l_norm"]])), axis=1)
              return res, pulled

          def build_elo(res):
              K,BASE=32.0,1500.0
              res=res.sort_values(["tour","date"]).reset_index(drop=True)
              elos={"ATP":{}, "WTA":{}}
              rows=[]
              for _,r in res.iterrows():
                  t,w,l=r["tour"],r["w_norm"],r["l_norm"]
                  if not w or not l: continue
                  ew,el=elos[t].get(w,BASE), elos[t].get(l,BASE)
                  expw=1/(1+10**((el-ew)/400)); elos[t][w]=ew+K*(1-expw); elos[t][l]=el+K*(0-(1-expw))
                  rows.append({"tour":t,"date":r["date"],"w_norm":w,"l_norm":l,"elo_w_pre":ew,"elo_l_pre":el})
              hist=pd.DataFrame(rows)
              pd.DataFrame([{"player":p,"elo":v} for p,v in elos["ATP"].items()]).to_csv(ATP_ELO,index=False)
              pd.DataFrame([{"player":p,"elo":v} for p,v in elos["WTA"].items()]).to_csv(WTA_ELO,index=False)
              return hist

          def load_odds():
              d=ODDS_PRIMARY if any(glob.glob(ODDS_PRIMARY+"/*.csv")) else ODDS_FALLBACK
              files=sorted(glob.glob(d+"/*.csv")+glob.glob(d+"/*.xlsx")+glob.glob(d+"/*.xls"))
              frames=[]
              for p in files:
                  try:
                      df=pd.read_excel(p) if p.lower().endswith((".xlsx",".xls")) else pd.read_csv(p)
                  except Exception as e:
                      print(f"[WARN] skip {p}: {e}"); continue
                  lower={c.lower():c for c in df.columns}
                  def pick(*opts):
                      for o in opts:
                          if o in lower: return lower[o]
                      return None
                  c_date=pick("date","event_date","match_date")
                  c_pa=pick("player_a","home","player1","p1","selection","player")
                  c_pb=pick("player_b","away","player2","p2","opponent")
                  c_oa=pick("odds_a","price_a","decimal_odds_a","odds1","home_odds","price1","best_odds_a")
                  c_ob=pick("odds_b","price_b","decimal_odds_b","odds2","away_odds","price2","best_odds_b")
                  if None in (c_date,c_pa,c_pb,c_oa,c_ob): continue
                  tmp=pd.DataFrame({
                      "date":pd.to_datetime(df[c_date], errors="coerce").dt.normalize(),
                      "a":df[c_pa].astype(str),"b":df[c_pb].astype(str),
                      "oa":pd.to_numeric(df[c_oa], errors="coerce"),"ob":pd.to_numeric(df[c_ob], errors="coerce")
                  }).dropna(subset=["date","oa","ob"])
                  frames.append(tmp)
              return (pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(), d, files)

          def link(res, elo, odds):
              out_df=pd.DataFrame(columns=OUTPUT_COLS)
              if odds is None or odds.empty: return out_df, pd.DataFrame()

              res_small=res[["tour","date","tourney_name","round","w_norm","l_norm","pair","surface"]].copy()
              unmatched=[]; linked=[]
              for _,o in odds.iterrows():
                  d0=o["date"]; pair=" vs ".join(sorted([norm(o["a"]), norm(o["b"])]))
                  win = res_small[(res_small["pair"]==pair) &
                                  (res_small["date"].between(d0-pd.Timedelta(days=10), d0+pd.Timedelta(days=10)))]
                  if win.empty:
                      choices=res_small["pair"].tolist()
                      match,score,_=process.extractOne(pair, choices, scorer=fuzz.token_sort_ratio)
                      if score>=92:
                          win=res_small[(res_small["pair"]==match) &
                                        (res_small["date"].between(d0-pd.Timedelta(days=10), d0+pd.Timedelta(days=10)))]
                  if win.empty:
                      unmatched.append({"date":d0, "player_a":o["a"], "player_b":o["b"]}); continue

                  m=win.sort_values("date", key=lambda s:(s-d0).abs()).iloc[0]
                  e_sub=elo[(elo["tour"]==m["tour"])&(elo["date"]<=m["date"])]

                  def last(p,side):
                      col="elo_w_pre" if side=="w" else "elo_l_pre"
                      dfp=e_sub[e_sub[f"{side}_norm"]==p]
                      return dfp[col].iloc[-1] if not dfp.empty else 1500.0
                  elo_w,elo_l=last(m["w_norm"],"w"), last(m["l_norm"],"l")

                  pn = norm(o["a"])
                  res_val = 1 if pn==m["w_norm"] else 0
                  linked.append({
                      "date":m["date"].date(),"tour":m["tour"],"tournament":m["tourney_name"],"round":m["round"],
                      "player":o["a"],"opponent":o["b"],"odds":o["oa"],"opp_odds":o["ob"],"result":res_val,
                      "elo_player": (elo_w if pn==m["w_norm"] else elo_l),
                      "elo_opponent": (elo_l if pn==m["w_norm"] else elo_w),
                      "surface":m["surface"],"source":"normalized"
                  })
              if linked:
                  out_df=pd.DataFrame(linked, columns=OUTPUT_COLS)
              return out_df, pd.DataFrame(unmatched)

          res,pulled=fetch_sack()
          elo=build_elo(res)
          odds, which_dir, files = load_odds()
          linked, unmatched = link(res, elo, odds)

          os.makedirs("data", exist_ok=True)
          linked.sort_values(["date","tour","tournament","player"], inplace=True, ignore_index=True)
          linked.to_csv(OUT_CSV, index=False)  # ALWAYS writes headers

          meta={"years":YEARS,"built_at_utc":datetime.utcnow().isoformat(timespec="seconds"),
                "rows":int(len(linked)), "date_min": (linked["date"].min().isoformat() if len(linked) else None),
                "date_max": (linked["date"].max().isoformat() if len(linked) else None)}
          with open(META,"w") as f: json.dump(meta,f)

          lines=[]
          lines.append("# Build summary\n")
          lines.append(f"- Years: **{YEARS}**")
          lines.append("- Sackmann pulls:")
          for (tour,y),n in sorted(pulled.items()):
              lines.append(f"  - {tour} {y}: **{n:,}** rows")
          lines.append(f"- Local odds directory used: `{which_dir}`")
          lines.append(f"- Local odds files: **{len(files)}**")
          lines.append(f"- Local odds rows: **{0 if odds is None else len(odds):,}**")
          lines.append(f"- Linked rows written: **{len(linked):,}** → `data/historical_matches.csv`")

          if len(linked):
              lines.append(f"- Dataset date range: **{linked['date'].min()} → {linked['date'].max()}**")
              lines.append(f"- Unique players: **{linked['player'].nunique()}**")
              head=linked.head(10).to_markdown(index=False)
              lines.append("\n<details><summary>Sample (first 10 rows)</summary>\n\n"+head+"\n\n</details>")
          elif not unmatched.empty:
              miss = unmatched.head(10).to_markdown(index=False)
              lines.append("\n<details><summary>Unmatched sample (first 10)</summary>\n\n"+miss+"\n\n</details>")
          else:
              lines.append("> No linked rows and no unmatched sample available.")

          with open(BUILD_MD,"w") as f: f.write("\n".join(lines))
          print("[DONE] Build summary saved.")
          PY

      - name: Publish summaries
        run: |
          if [ -f data/normalize_report.md ]; then
            echo "## Normalize report" >> "$GITHUB_STEP_SUMMARY"
            cat data/normalize_report.md >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -f data/build_summary.md ]; then
            echo "## Build summary" >> "$GITHUB_STEP_SUMMARY"
            cat data/build_summary.md >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload dataset artifact
        uses: actions/upload-artifact@v4
        with:
          name: courtsense-dataset
          path: |
            data/historical_matches.csv
            data/atp_elo.csv
            data/wta_elo.csv
            data/dataset_info.json
            data/build_summary.md
            data/normalize_report.md
          if-no-files-found: warn

  backtest:
    needs: build
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.do_backtest == 'yes'
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download dataset
        uses: actions/download-artifact@v4
        with:
          name: courtsense-dataset
          path: data_bundle

      - name: Stage dataset
        run: |
          mkdir -p data
          cp -f data_bundle/historical_matches.csv data/historical_matches.csv || true
          cp -f data_bundle/atp_elo.csv           data/atp_elo.csv || true
          cp -f data_bundle/wta_elo.csv           data/wta_elo.csv || true

      - name: Setup Python
        uses: actions/setup-python@v5
        with: { python-version: "3.11" }

      - name: Install runtime deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy

      - name: Ensure injuries.json exists (stub)
        run: |
          test -f injuries.json || echo '[]' > injuries.json

      - name: Parse inputs; run TE8 backtest
        shell: bash
        run: |
          test -f backtest_te8.py || { echo "::error ::backtest_te8.py missing in repo root"; exit 1; }

          BANDS="${{ inputs.bands }}"
          DOG_BAND=$(echo "$BANDS" | sed -n 's/.*dog=\([^;]*\).*/\1/p'); [ -z "$DOG_BAND" ] && DOG_BAND="2.20,4.50"
          FAV_BAND=$(echo "$BANDS" | sed -n 's/.*fav=\([^;]*\).*/\1/p'); [ -z "$FAV_BAND" ] && FAV_BAND="1.15,2.00"

          MONEY="${{ inputs.money }}"
          BANKROLL=$(echo "$MONEY" | sed -n 's/.*bankroll=\([^;]*\).*/\1/p');   [ -z "$BANKROLL" ] && BANKROLL="1000"
          STAKE_UNIT=$(echo "$MONEY" | sed -n 's/.*stake_unit=\([^;]*\).*/\1/p');[ -z "$STAKE_UNIT" ] && STAKE_UNIT="100"
          DOG_CAP=$(echo "$MONEY" | sed -n 's/.*dog_cap=\([^;]*\).*/\1/p');     [ -z "$DOG_CAP" ] && DOG_CAP="0.25"

          TUNE="${{ inputs.tuners }}"
          SURF_BOOST=$(echo "$TUNE" | sed -n 's/.*surface_boost=\([^;]*\).*/\1/p');         [ -z "$SURF_BOOST" ] && SURF_BOOST="0.05"
          FORM_W=$(echo "$TUNE" | sed -n 's/.*recent_form_weight=\([^;]*\).*/\1/p');        [ -z "$FORM_W" ] && FORM_W="0.30"
          INJ_PEN=$(echo "$TUNE" | sed -n 's/.*injury_penalty=\([^;]*\).*/\1/p');           [ -z "$INJ_PEN" ] && INJ_PEN="0.15"

          python backtest_te8.py \
            --input data/historical_matches.csv \
            --elo-atp data/atp_elo.csv \
            --elo-wta data/wta_elo.csv \
            --injuries injuries.json \
            --start "${{ inputs.start_date }}" \
            --end   "${{ inputs.end_date }}" \
            --te8-dog "${{ inputs.te8_dog }}" \
            --te8-fav "${{ inputs.te8_fav }}" \
            --dog-band "$DOG_BAND" \
            --fav-band "$FAV_BAND" \
            --dog-cap "$DOG_CAP" \
            --stake-unit "$STAKE_UNIT" \
            --bankroll "$BANKROLL" \
            --surface-boost "$SURF_BOOST" \
            --recent-form-weight "$FORM_W" \
            --injury-penalty "$INJ_PEN" \
            --out-csv backtest_results.csv \
            --summary backtest_summary.md

      - name: Publish summaries
        if: always()
        run: |
          if [ -f data_bundle/build_summary.md ]; then
            echo "## Build summary" >> "$GITHUB_STEP_SUMMARY"
            cat data_bundle/build_summary.md >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -f backtest_summary.md ]; then
            cat backtest_summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No backtest summary produced._" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: backtest-output
          path: |
            backtest_results.csv
            backtest_summary.md
          if-no-files-found: warn
